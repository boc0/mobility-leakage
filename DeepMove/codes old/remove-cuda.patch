diff --git a/codes/main.py b/codes/main.py
index c568559..270a749 100644
--- a/codes/main.py
+++ b/codes/main.py
@@ -40,11 +40,11 @@ def run(args):
         parameters.model_mode, parameters.history_mode, parameters.uid_size))
 
     if parameters.model_mode in ['simple', 'simple_long']:
-        model = TrajPreSimple(parameters=parameters).cuda()
+        model = TrajPreSimple(parameters)
     elif parameters.model_mode == 'attn_avg_long_user':
-        model = TrajPreAttnAvgLongUser(parameters=parameters).cuda()
+        model = TrajPreAttnAvgLongUser(parameters)
     elif parameters.model_mode == 'attn_local_long':
-        model = TrajPreLocalAttnLong(parameters=parameters).cuda()
+        model = TrajPreLocalAttnLong(parameters)
     if args.pretrain == 1:
         model.load_state_dict(torch.load("../pretrain/" + args.model_mode + "/res.m"))
 
@@ -55,7 +55,7 @@ def run(args):
     else:
         parameters.history_mode = 'whole'
 
-    criterion = nn.NLLLoss().cuda()
+    criterion = nn.NLLLoss()
     optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=parameters.lr,
                            weight_decay=parameters.L2)
     scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=parameters.lr_step,
@@ -181,7 +181,6 @@ class Settings(object):
 if __name__ == '__main__':
     np.random.seed(1)
     torch.manual_seed(1)
-    os.environ["CUDA_VISIBLE_DEVICES"] = "0"
 
     parser = argparse.ArgumentParser()
     parser.add_argument('--loc_emb_size', type=int, default=500, help="location embeddings size")
diff --git a/codes/model.py b/codes/model.py
index 03a1d26..403489d 100644
--- a/codes/model.py
+++ b/codes/model.py
@@ -98,11 +98,11 @@ class Attn(nn.Module):
     def forward(self, out_state, history):
         seq_len = history.size()[0]
         state_len = out_state.size()[0]
-        attn_energies = Variable(torch.zeros(state_len, seq_len)).cuda()
+        attn_energies = Variable(torch.zeros(state_len, seq_len))
         for i in range(state_len):
             for j in range(seq_len):
                 attn_energies[i, j] = self.score(out_state[i], history[j])
-        return F.softmax(attn_energies)
+        return F.softmax(attn_energies, dim=1)
 
     def score(self, hidden, encoder_output):
         if self.method == 'dot':
@@ -173,8 +173,8 @@ class TrajPreAttnAvgLongUser(nn.Module):
         h1 = Variable(torch.zeros(1, 1, self.hidden_size))
         c1 = Variable(torch.zeros(1, 1, self.hidden_size))
         if self.use_cuda:
-            h1 = h1.cuda()
-            c1 = c1.cuda()
+            h1 = h1
+            c1 = c1
 
         loc_emb = self.emb_loc(loc)
         tim_emb = self.emb_tim(tim)
@@ -184,8 +184,8 @@ class TrajPreAttnAvgLongUser(nn.Module):
         loc_emb_history = self.emb_loc(history_loc).squeeze(1)
         tim_emb_history = self.emb_tim(history_tim).squeeze(1)
         count = 0
-        loc_emb_history2 = Variable(torch.zeros(len(history_count), loc_emb_history.size()[-1])).cuda()
-        tim_emb_history2 = Variable(torch.zeros(len(history_count), tim_emb_history.size()[-1])).cuda()
+        loc_emb_history2 = Variable(torch.zeros(len(history_count), loc_emb_history.size()[-1]))
+        tim_emb_history2 = Variable(torch.zeros(len(history_count), tim_emb_history.size()[-1]))
         for i, c in enumerate(history_count):
             if c == 1:
                 tmp = loc_emb_history[count].unsqueeze(0)
diff --git a/codes/train.py b/codes/train.py
index e67daea..6e34624 100644
--- a/codes/train.py
+++ b/codes/train.py
@@ -34,7 +34,7 @@ class RnnParameterData(object):
 
         self.epoch = epoch_max
         self.dropout_p = dropout_p
-        self.use_cuda = True
+        self.use_cuda = False
         self.lr = lr
         self.lr_step = lr_step
         self.lr_decay = lr_decay
@@ -300,45 +300,38 @@ def run_simple(data, run_idx, mode, lr, clip, model, optimizer, criterion, mode2
         optimizer.zero_grad()
         u, i = run_queue.popleft()
         if u not in users_acc:
-            users_acc[u] = [0, 0]
-        loc = data[u][i]['loc'].cuda()
-        tim = data[u][i]['tim'].cuda()
-        target = data[u][i]['target'].cuda()
-        uid = Variable(torch.LongTensor([u])).cuda()
+            users_acc[u] = []
+
+        loc = data[u][i]['loc']
+        tim = data[u][i]['tim']
+        target = data[u][i]['target']
+        uid = Variable(torch.LongTensor([u]))
 
         if 'attn' in mode2:
-            history_loc = data[u][i]['history_loc'].cuda()
-            history_tim = data[u][i]['history_tim'].cuda()
+            history_loc = data[u][i]['history_loc']
+            history_tim = data[u][i]['history_tim']
+            history_count = data[u][i]['history_count']
 
         if mode2 in ['simple', 'simple_long']:
             scores = model(loc, tim)
         elif mode2 == 'attn_avg_long_user':
-            history_count = data[u][i]['history_count']
-            target_len = target.data.size()[0]
-            scores = model(loc, tim, history_loc, history_tim, history_count, uid, target_len)
+            scores = model(loc, tim, history_loc, history_tim, history_count, uid, target.data.size()[0])
         elif mode2 == 'attn_local_long':
-            target_len = target.data.size()[0]
-            scores = model(loc, tim, target_len)
+            scores = model(loc, tim, target.data.size()[0])
 
         if scores.data.size()[0] > target.data.size()[0]:
             scores = scores[-target.data.size()[0]:]
+
         loss = criterion(scores, target)
 
         if mode == 'train':
             loss.backward()
-            # gradient clipping
-            try:
-                torch.nn.utils.clip_grad_norm(model.parameters(), clip)
-                for p in model.parameters():
-                    if p.requires_grad:
-                        p.data.add_(-lr, p.grad.data)
-            except:
-                pass
+            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
+            torch.nn.utils.clip_grad_norm(model.parameters(), clip)
             optimizer.step()
         elif mode == 'test':
-            users_acc[u][0] += len(target)
-            acc = get_acc(target, scores)
-            users_acc[u][1] += acc[2]
+            users_acc[u].append(get_acc(target, scores))
+
         total_loss.append(loss.data.cpu().numpy()[0])
 
     avg_loss = np.mean(total_loss, dtype=np.float64)
@@ -347,9 +340,9 @@ def run_simple(data, run_idx, mode, lr, clip, model, optimizer, criterion, mode2
     elif mode == 'test':
         users_rnn_acc = {}
         for u in users_acc:
-            tmp_acc = users_acc[u][1] / users_acc[u][0]
-            users_rnn_acc[u] = tmp_acc.tolist()[0]
-        avg_acc = np.mean([users_rnn_acc[x] for x in users_rnn_acc])
+            user_acc = np.mean(np.array(users_acc[u]), axis=0)
+            users_rnn_acc[u] = user_acc.tolist()
+        avg_acc = np.mean(np.array([users_rnn_acc[u] for u in users_rnn_acc]), axis=0)
         return avg_loss, avg_acc, users_rnn_acc
 
 
