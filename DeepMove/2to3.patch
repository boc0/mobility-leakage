diff --git a/codes/main.py b/codes/main.py
index c568559..317cc38 100644
--- a/codes/main.py
+++ b/codes/main.py
@@ -1,6 +1,6 @@
 # coding: utf-8
-from __future__ import print_function
-from __future__ import division
+
+
 
 import torch
 import torch.nn as nn
@@ -56,7 +56,7 @@ def run(args):
         parameters.history_mode = 'whole'
 
     criterion = nn.NLLLoss().cuda()
-    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=parameters.lr,
+    optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=parameters.lr,
                            weight_decay=parameters.L2)
     scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=parameters.lr_step,
                                                      factor=parameters.lr_decay, threshold=1e-3)
@@ -64,7 +64,7 @@ def run(args):
     lr = parameters.lr
     metrics = {'train_loss': [], 'valid_loss': [], 'accuracy': [], 'valid_acc': {}}
 
-    candidate = parameters.data_neural.keys()
+    candidate = list(parameters.data_neural.keys())
     avg_acc_markov, users_acc_markov = markov(parameters, candidate)
     metrics['markov_acc'] = users_acc_markov
 
diff --git a/codes/model.py b/codes/model.py
index 03a1d26..86b3d77 100644
--- a/codes/model.py
+++ b/codes/model.py
@@ -1,6 +1,6 @@
 # coding: utf-8
-from __future__ import print_function
-from __future__ import division
+
+
 
 import torch
 import torch.nn as nn
diff --git a/codes/sparse_traces.py b/codes/sparse_traces.py
index 2a0011a..e9ea1ef 100644
--- a/codes/sparse_traces.py
+++ b/codes/sparse_traces.py
@@ -1,10 +1,10 @@
-from __future__ import print_function
-from __future__ import division
+
+
 
 import time
 import argparse
 import numpy as np
-import cPickle as pickle
+import pickle as pickle
 from collections import Counter
 
 
diff --git a/codes/train.py b/codes/train.py
index e67daea..a92c38c 100644
--- a/codes/train.py
+++ b/codes/train.py
@@ -1,12 +1,12 @@
 # coding: utf-8
-from __future__ import print_function
-from __future__ import division
+
+
 
 import torch
 from torch.autograd import Variable
 
 import numpy as np
-import cPickle as pickle
+import pickle as pickle
 from collections import deque, Counter
 
 
@@ -52,7 +52,7 @@ def generate_input_history(data_neural, mode, mode2=None, candidate=None):
     data_train = {}
     train_idx = {}
     if candidate is None:
-        candidate = data_neural.keys()
+        candidate = list(data_neural.keys())
     for u in candidate:
         sessions = data_neural[u]['sessions']
         train_id = data_neural[u][mode]
@@ -132,7 +132,7 @@ def generate_input_long_history2(data_neural, mode, candidate=None):
     data_train = {}
     train_idx = {}
     if candidate is None:
-        candidate = data_neural.keys()
+        candidate = list(data_neural.keys())
     for u in candidate:
         sessions = data_neural[u]['sessions']
         train_id = data_neural[u][mode]
@@ -164,7 +164,7 @@ def generate_input_long_history(data_neural, mode, candidate=None):
     data_train = {}
     train_idx = {}
     if candidate is None:
-        candidate = data_neural.keys()
+        candidate = list(data_neural.keys())
     for u in candidate:
         sessions = data_neural[u]['sessions']
         train_id = data_neural[u][mode]
@@ -217,7 +217,7 @@ def generate_input_long_history(data_neural, mode, candidate=None):
 
 def generate_queue(train_idx, mode, mode2):
     """return a deque. You must use it by train_queue.popleft()"""
-    user = train_idx.keys()
+    user = list(train_idx.keys())
     train_queue = deque()
     if mode == 'random':
         initial_queue = {}
@@ -375,7 +375,7 @@ def markov(parameters, candidate):
     acc = 0
     count = 0
     user_acc = {}
-    for u in validation.keys():
+    for u in list(validation.keys()):
         topk = list(set(validation[u][0]))
         transfer = np.zeros((len(topk), len(topk)))
 
